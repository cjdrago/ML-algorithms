{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Document Classification with KNN and Naive Bayes\n",
    "\n",
    "You will classify text documents using Naive Bayes classifers.  We will be working with the dataset called \"20 Newsgroups\", which is a collection of 20,000 newsgroup posts organized into 20 categories."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Loading the 20 Newsgroups Dataset\n",
    "The dataset is called “20 Newsgroups”. Here is the official description, quoted from the [website](http://qwone.com/~jason/20Newsgroups/)\n",
    ">The 20 Newsgroups data set is a collection of approximately 20,000 newsgroup documents, partitioned (nearly) evenly across 20 different newsgroups. To the best of our knowledge, it was originally collected by Ken Lang, probably for his paper “Newsweeder: Learning to filter netnews,” though he does not explicitly mention this collection. The 20 newsgroups collection has become a popular data set for experiments in text applications of machine learning techniques, such as text classification and text clustering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#First we need to initialize Python.  Run the below cell.\n",
    "%matplotlib inline\n",
    "import IPython.core.display         \n",
    "# setup output image format (Chrome works best)\n",
    "IPython.core.display.set_matplotlib_formats(\"svg\")\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "from sklearn.pipeline import Pipeline\n",
    "import operator\n",
    "\n",
    "from numpy import *\n",
    "from sklearn import *\n",
    "from scipy import stats\n",
    "random.seed(4487)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Put the file \"20news-bydate_py3.pkz' into the same directory as this ipynb file. **Do not unzip the file**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Extract 4 classes ('alt.atheism', 'talk.religion.misc', 'comp.graphics', 'sci.space') from the dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# strip away headers/footers/quotes from the text\n",
    "removeset = ('headers', 'footers', 'quotes')\n",
    "\n",
    "# only use 4 categories\n",
    "cats      = ['alt.atheism', 'talk.religion.misc', 'comp.graphics', 'sci.space']\n",
    "\n",
    "# load the training and testing sets\n",
    "newsgroups_train = datasets.fetch_20newsgroups(subset='train',\n",
    "                           remove=removeset, categories=cats, data_home='./')\n",
    "newsgroups_test  = datasets.fetch_20newsgroups(subset='test', \n",
    "                           remove=removeset, categories=cats, data_home='./')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Check if we got all the data.  The training set should have 2034 documents, and the test set should have 1353 documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training set size: 2034\n",
      "testing set size:  1353\n",
      "['alt.atheism', 'comp.graphics', 'sci.space', 'talk.religion.misc']\n"
     ]
    }
   ],
   "source": [
    "print(\"training set size:\", len(newsgroups_train.data))\n",
    "print(\"testing set size: \",  len(newsgroups_test.data))\n",
    "print(newsgroups_train.target_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Count the number examples in each class.  `newsgroups_train.target` is an array of class values (0 through 3), and `newsgroups_train.target[i]` is the class of the i-th document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class counts\n",
      "alt.atheism         : 480\n",
      "comp.graphics       : 584\n",
      "sci.space           : 593\n",
      "talk.religion.misc  : 377\n"
     ]
    }
   ],
   "source": [
    "print(\"class counts\")\n",
    "for i in [0, 1, 2, 3]:\n",
    "    print(\"{:20s}: {}\".format(newsgroups_train.target_names[i], sum(newsgroups_train.target == i)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Show the documents.  `newsgroups_train.data` is a list of strings, and `newsgroups_train.data[i]` is the i-th document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- document 0 (class=comp.graphics) ---\n",
      "Hi,\n",
      "\n",
      "I've noticed that if you only save a model (with all your mapping planes\n",
      "positioned carefully) to a .3DS file that when you reload it after restarting\n",
      "3DS, they are given a default position and orientation.  But if you save\n",
      "to a .PRJ file their positions/orientation are preserved.  Does anyone\n",
      "know why this information is not stored in the .3DS file?  Nothing is\n",
      "explicitly said in the manual about saving texture rules in the .PRJ file. \n",
      "I'd like to be able to read the texture rule information, does anyone have \n",
      "the format for the .PRJ file?\n",
      "\n",
      "Is the .CEL file format available from somewhere?\n",
      "\n",
      "Rych\n"
     ]
    }
   ],
   "source": [
    "print(\"--- document {} (class={}) ---\".format(\n",
    "    0, newsgroups_train.target_names[newsgroups_train.target[0]]))\n",
    "print(newsgroups_train.data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tip:** while you do the tutorial, it is okay to make additional code cells in the file.  This will allow you to avoid re-running code (like training a classifier, then testing a classifier)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Extracting Features from Text Files\n",
    "In order to perform machine learning on text documents, we first need to turn the text content into numerical feature vectors.\n",
    "\n",
    "Next, we will introduce two basic text representation methods: One-hot encoding, Bag of words, and TF-IDF. More feature vector extraction functions, please refer to https://scikit-learn.org/stable/modules/classes.html#module-sklearn.feature_extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### one-hot encoding\n",
    "- Each word is coded with an index, which is represented by one-hot."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> John likes to watch movies. Mary likes too.\n",
    "\n",
    "> John also likes to watch football games."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we need to represent the words in the above two sentences, you can encode the words as following:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> {\"John\": 1, \"likes\": 2, \"to\": 3, \"watch\": 4, \"movies\": 5, \"also\":6, \"football\": 7, \"games\": 8, \"Mary\": 9, \"too\": 10}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can encode each word using one-hot method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">John: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    "\n",
    ">likes: [0, 1, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    "\n",
    ">..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### However, this text representation method is impractical when the scale of corpus becomes large."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bag of Words\n",
    "- The index value of a word in the vocabulary is linked to its frequency in the whole training corpus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> John likes to watch movies. Mary likes too.  -->> [1, 2, 1, 1, 1, 0, 0, 0, 1, 1]\n",
    "\n",
    "> John also likes to watch football games.     -->> [1, 1, 1, 1, 0, 1, 1, 1, 0, 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **sklearn.feature_extraction.text.CountVectorizer** implement the `Bag of Words` method that converts a collection of text documents to a matrix of token counts. This implementation produces a sparse representation of the counts using **scipy.sparse.coo_matrix** to save memory by only storing the non-zero parts of the feature vectors in memory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Term Frequency - Inverse Document Frequency (TF-IDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the word bag model, we can get the vector representation of this text.However, in the face of the diversity of text, each word has different weight to the content of text in practical application, so we introduce tf-idf model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### TF (Term Frequency)\n",
    "\n",
    "In the case of the term frequency $tf(t, d)$, the simplest choice is to use the raw count of a term in a document, i.e., the number of times that term $t$ occurs in document $d$. If we denote the raw count by $f_{t, d}$, then the simplest tf scheme is $tf(t,d) = f_{t, d}$. \n",
    "\n",
    "$tf_{t, d} = n_{t, d}/\\sum_kn_{t, d}$ (divided)\n",
    "\n",
    "The numerator in the above formula is the number of occurrences of the word in the document $d$, and the denominator is the sum of the occurrences of all words in the document $d$.\n",
    "\n",
    "##### IDF (Inverse Document Frequency) \n",
    "\n",
    "The inverse document frequency is a measure of how much information the word provides, i.e., if it's common or rare across all documents. It is the logarithmically scaled inverse fraction of the documents that contain the word (obtained by dividing the total number of documents by the number of documents containing the term, and then taking the logarithm of that quotient): \n",
    "\n",
    "$idf(t ,D) = log\\frac{N}{|\\{ d\\in D:t \\in d \\}|}$\n",
    "\n",
    "with \n",
    "- $N$: total number of documents in the corpus $N=|D|$\n",
    "- $|\\{ d\\in D:t \\in d \\}|$: number of documents where the term $t$ appears. If the term is not in the corpus, this will lead to a division-by-zero. It is therefore common to adjust the denominator to  $1+|\\{ d\\in D:t \\in d \\}|$\n",
    "\n",
    "Then tf-idf is calculated as: \n",
    "$tfidf(t, d, D) = tf(t, d) * idf(t, D)$\n",
    "\n",
    "Both tf and tf–idf can be computed as follows using **sklearn.feature_extraction.text.TfidfTransformer**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 1 1 1 0 0 1 0 1]\n",
      " [0 2 0 1 0 1 1 0 1]\n",
      " [1 0 0 1 1 0 1 1 1]\n",
      " [0 1 1 1 0 0 1 0 1]]\n",
      "[1.91629073 1.22314355 1.51082562 1.         1.91629073 1.91629073\n",
      " 1.         1.91629073 1.        ]\n",
      "['and', 'document', 'first', 'is', 'one', 'second', 'the', 'third', 'this']\n",
      "[[0.         0.46979139 0.58028582 0.38408524 0.         0.\n",
      "  0.38408524 0.         0.38408524]\n",
      " [0.         0.6876236  0.         0.28108867 0.         0.53864762\n",
      "  0.28108867 0.         0.28108867]\n",
      " [0.51184851 0.         0.         0.26710379 0.51184851 0.\n",
      "  0.26710379 0.51184851 0.26710379]\n",
      " [0.         0.46979139 0.58028582 0.38408524 0.         0.\n",
      "  0.38408524 0.         0.38408524]]\n"
     ]
    }
   ],
   "source": [
    "# This is Token count (Simple bag of words)\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer, TfidfVectorizer\n",
    " \n",
    "corpus_example = ['This is the first document.',\n",
    "\t'This document is the second document.',\n",
    "\t'And this is the third one.',\n",
    "\t'Is this the first document?']\n",
    " \n",
    "pipe_example = Pipeline([('bag_of_words', CountVectorizer()),\n",
    "                 ('tfid', TfidfTransformer())]).fit(corpus_example)\n",
    "print(pipe_example['bag_of_words'].transform(corpus_example).toarray())\n",
    "print(pipe_example['tfid'].idf_)\n",
    "results_tfid_example = pipe_example.transform(corpus_example)\n",
    "print(pipe_example['bag_of_words'].get_feature_names())\n",
    "print(results_tfid_example.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the vocabulary from the training data.  Then use **sklearn.feature_extraction.text.CountVectorizer** to build the document vectors for the training and testing sets.  You can decide how many words you want in the vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2034, 26576)\n"
     ]
    }
   ],
   "source": [
    "vectorizer = feature_extraction.text.CountVectorizer(stop_words='english')\n",
    "X_train = vectorizer.fit_transform(newsgroups_train['data'])\n",
    "print(X_train.toarray().shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('ysc', 26475),\n",
       " ('yr', 26474),\n",
       " ('yoyodyne', 26473),\n",
       " ('youve', 26472),\n",
       " ('youth', 26471),\n",
       " ('yousung', 26470),\n",
       " ('yourselfers', 26469),\n",
       " ('yourdon', 26468),\n",
       " ('youngsters', 26467),\n",
       " ('youngster', 26466),\n",
       " ('young', 26465),\n",
       " ('yoshiro', 26464),\n",
       " ('yorku', 26463),\n",
       " ('york', 26462),\n",
       " ('yorg', 26461),\n",
       " ('yorba', 26460),\n",
       " ('yor', 26459),\n",
       " ('yoke', 26458),\n",
       " ('yo', 26457),\n",
       " ('yngvesson', 26456),\n",
       " ('yktvmh', 26455),\n",
       " ('yingyong', 26454),\n",
       " ('yields', 26453),\n",
       " ('yielding', 26452),\n",
       " ('yielded', 26451),\n",
       " ('yield', 26450),\n",
       " ('yhwh', 26449),\n",
       " ('yhvh', 26448),\n",
       " ('yfn', 26447),\n",
       " ('yf', 26446),\n",
       " ('yesterday', 26445),\n",
       " ('yes', 26444),\n",
       " ('yer', 26443),\n",
       " ('yep', 26442),\n",
       " ('yeltsin', 26441),\n",
       " ('yellows', 26440),\n",
       " ('yellow', 26439),\n",
       " ('yelling', 26438),\n",
       " ('yell', 26437),\n",
       " ('yee', 26436),\n",
       " ('yeasteryears', 26435),\n",
       " ('yeast', 26434),\n",
       " ('years', 26433),\n",
       " ('yearly', 26432),\n",
       " ('year', 26431),\n",
       " ('yeah', 26430),\n",
       " ('yeager', 26429),\n",
       " ('yea', 26428),\n",
       " ('ye', 26427),\n",
       " ('ydobyns', 26426)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(vectorizer.vocabulary_.items(), key=operator.itemgetter(0), reverse = True)[100:150]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('0674', 100),\n",
       " ('068', 101),\n",
       " ('0695', 102),\n",
       " ('07', 103),\n",
       " ('070', 104),\n",
       " ('071', 105),\n",
       " ('0729', 106),\n",
       " ('0739', 107),\n",
       " ('074', 108),\n",
       " ('07410', 109),\n",
       " ('07653', 110),\n",
       " ('077', 111),\n",
       " ('08', 112),\n",
       " ('08080', 113),\n",
       " ('0820', 114),\n",
       " ('08540', 115),\n",
       " ('08544', 116),\n",
       " ('0856e16', 117),\n",
       " ('0865', 118),\n",
       " ('08786', 119),\n",
       " ('08934', 120),\n",
       " ('09', 121),\n",
       " ('0900', 122),\n",
       " ('0901', 123),\n",
       " ('0903', 124),\n",
       " ('0908', 125),\n",
       " ('0926', 126),\n",
       " ('0941', 127),\n",
       " ('0943', 128),\n",
       " ('0970', 129),\n",
       " ('0971', 130),\n",
       " ('0988', 131),\n",
       " ('0_', 132),\n",
       " ('0a', 133),\n",
       " ('0b', 134),\n",
       " ('0e9', 135),\n",
       " ('0km', 136),\n",
       " ('0mph', 137),\n",
       " ('0w', 138),\n",
       " ('0x', 139),\n",
       " ('0x00', 140),\n",
       " ('0x100', 141),\n",
       " ('0x1f', 142),\n",
       " ('0x3d4', 143),\n",
       " ('0xc010', 144),\n",
       " ('0xc018', 145),\n",
       " ('10', 146),\n",
       " ('100', 147),\n",
       " ('1000', 148),\n",
       " ('10000', 149)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(vectorizer.vocabulary_.items(), key=operator.itemgetter(0))[100:150]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete words that doesnt have a count token higher than 200\n",
    "# I did this trying to eliminate outliers and unsignificant words such as '00', '0000' or '3pm'.\n",
    "# Also words that are repeated in every text doesnt have many importance for us, therefore we want to delete the most common words\n",
    "# 1. 18.000 > x > 1000\n",
    "new_vocabulary = dict(filter(lambda dict_elements: 20000 > dict_elements[1] > 2000,vectorizer.vocabulary_.items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = feature_extraction.text.CountVectorizer(stop_words='english', vocabulary = new_vocabulary.keys())\n",
    "X_train = vectorizer.fit_transform(newsgroups_train['data'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. K Nearest Neighbor (KNN)\n",
    "Let's train a K Nearest Neighbor (KNN) model. Using cross-validation to select the best K parameter. Then, showing the accuracy of training and testing set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2034, 17999)\n",
      "Accuracy:  0.471030136192408\n",
      "Params:  {'n_neighbors': 3, 'weights': 'distance'}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.neighbors import KNeighborsClassifier as knn\n",
    "\n",
    "Y_train = newsgroups_train['target']\n",
    "\n",
    "paramgrid = {'n_neighbors': [3,4,5,7,10],\n",
    "            'weights': ['distance']}\n",
    "grid_search = GridSearchCV(knn(), param_grid=paramgrid, cv=10, n_jobs=-1, scoring='accuracy')\n",
    "\n",
    "    \n",
    "print(X_train.shape)\n",
    "    \n",
    "grid_search.fit(X_train.toarray(), Y_train)\n",
    "print('Accuracy: ' , grid_search.best_score_)\n",
    "print('Params: ', grid_search.best_params_)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy of train set = 0.471"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "Y_test = newsgroups_test['target']\n",
    "X_test = vectorizer.transform(newsgroups_test['data']).toarray()\n",
    "\n",
    "Y_pred = grid_search.predict(X_test)\n",
    "accuracy_score(Y_test, Y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy of test set = 0.433"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Bernoulli Naive Bayes \n",
    "Learn a Bernoulli Naive Bayes model from the training set.  What is the prediction accuracy on the test set?  Try different parameters (alpha, max_features, etc) to get the best performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import BernoulliNB\n",
    "\n",
    "paramgrid = {'alpha' : [.01, .05, 0.1, 0.2]}\n",
    "grid_search = GridSearchCV(BernoulliNB(), param_grid=paramgrid, cv=10, n_jobs=-1, scoring='accuracy')\n",
    "\n",
    "grid_search.fit(X_train.toarray(), Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Prediction accuracy test set: {grid_search.best_score_}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(grid_search.predict(X_test), Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search.best_params_ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What are the most informative words for each category?  Run the below code.\n",
    "\n",
    "Note: `model.coef_[i]` will index the scores for the i-th class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bmodel = BernoulliNB(alpha = 0.01)\n",
    "bmodel.fit(X_train.toarray(), Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fnames = asarray(vectorizer.get_feature_names())\n",
    "for i,c in enumerate(newsgroups_train.target_names):\n",
    "    tmp = argsort(bmodel.coef_[i])[-10:]\n",
    "    print(\"class\", c)\n",
    "    for t in tmp:\n",
    "        print(\"    {:9s} ({:.5f})\".format(fnames[t], bmodel.coef_[i][t]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Multinomial Naive Bayes model\n",
    "Now learn a multinomial Naive Bayes model using the TF-IDF representation for the documents.  Again try different parameter values to improve the test accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### INSERT YOUR CODE HERE\n",
    "## HINT \n",
    "# 1. feature_extraction.text.TfidfTransformer(use_idf=True, norm= )\n",
    "# 2. naive_bayes.MultinomialNB(alpha= )\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "pipe_tfidf = Pipeline([('bag_of_words', CountVectorizer(vocabulary  = new_vocabulary.keys())),\n",
    "                     ('tf-idf', TfidfTransformer())]).fit(newsgroups_train['data'])\n",
    "\n",
    "X_train = pipe_tfidf.transform(newsgroups_train['data']).toarray()\n",
    "X_test = pipe_tfidf.transform(newsgroups_test['data']).toarray()\n",
    "print(X_train.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paramgrid = {'alpha' : [.01, .05, 0.1, 0.2],\n",
    "            'fit_prior': [1, 0]}\n",
    "grid_search = GridSearchCV(MultinomialNB(), param_grid=paramgrid, cv=10, n_jobs=-1, scoring='accuracy')\n",
    "\n",
    "grid_search.fit(X_train, Y_train)\n",
    "print(grid_search.best_score_)\n",
    "print(grid_search.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_MNB = MultinomialNB(alpha=grid_search.best_params_['alpha'], \n",
    "                          fit_prior = grid_search.best_params_['fit_prior']\n",
    "                         ).fit(X_train, newsgroups_train.target)\n",
    "Y_predict = model_MNB.predict(X_test)\n",
    "print(accuracy_score(newsgroups_test.target, Y_predict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What are the most informative features for Multinomial model? Run the below code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the word names\n",
    "fnames = asarray(pipe_tfidf['bag_of_words'].get_feature_names())\n",
    "for i,c in enumerate(newsgroups_train.target_names):\n",
    "    tmp = argsort(model_MNB.coef_[i])[-10:]\n",
    "    print(\"class\", c)\n",
    "    for t in tmp:\n",
    "        print(\"    {:9s} ({:.5f})\".format(fnames[t], model_MNB.coef_[i][t]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How do the most informative words differ between the TF-IDF multinomial model and the Bernoulli model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a bigg difference in the most informative words between the two models.\n",
    "Looking at the class labels and most improtant words of each one, We can say that the Bernulli model's words represent more the class than the words obtained from TF-IDF. That could be seen in the level of importance of each word: in Bernulli it is between -1.x and -2.x in most cases and in Multinominal its -5.x or -4.x.\n",
    "Also, in Bernulli model we can see words that make more sense with the class label where the text belongs. For example:\n",
    "1. alt.atheism: atheist, islam, atheism, god (in both models), and believe (in both models).\n",
    "2. comp.graphics: file, files, image, graphics (all last four in both models), and format.\n",
    "3. sci.space: earth, lunar, launch, data.\n",
    "4. talk.religion.misc: christians, bible, believe (in both models), jesus, god (in both models).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Effect of smoothing\n",
    "The smoothing (regularization) parameter has a big effect on the performance.  Using the Multinomial TF-IDF models, make a plot of accuracy versus different values of alpha. For each alpha, you need to train a new model. Which alpha value yields the best result?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### INSERT YOUR CODE HERE\n",
    "## HINT\n",
    "# 1. Iterating: alphas = logspace(-5,0,50)\n",
    "import numpy as np\n",
    "\n",
    "model_accuracy_alpha = []\n",
    "for alpha_param in logspace(-10,0,30):\n",
    "    \n",
    "    model_MNB = MultinomialNB(alpha= alpha_param).fit(X_train, newsgroups_train.target)\n",
    "    Y_predict = model_MNB.predict(X_test)\n",
    "    curr_accuracy = accuracy_score(newsgroups_test.target, Y_predict)\n",
    "    model_accuracy_alpha.append([curr_accuracy, alpha_param])\n",
    "      \n",
    "model_accuracy_alpha = np.array(model_accuracy_alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(model_accuracy_alpha[:,1], model_accuracy_alpha[:,0])\n",
    "plt.xlabel('Alpha Param')\n",
    "plt.ylabel('Model accuracy')\n",
    "plt.title('Model accuracy vs Alpha smoothing value')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Effect of vocabulary size\n",
    "The vocabulary size also affects the accuracy.  Make another plot of accuracy versus vocabulary size.  Which vocabulary size yields the best result?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_accuracy_vocabulary = []\n",
    "\n",
    "for vocabulary_size in linspace(100,26577,20):\n",
    "    vectorizer = TfidfVectorizer(max_features = int(vocabulary_size))\n",
    "    \n",
    "    X_train = vectorizer.fit_transform(newsgroups_train.data)\n",
    "    X_test = vectorizer.transform(newsgroups_test.data)\n",
    "    \n",
    "    model_MNB = MultinomialNB(alpha= 0.01).fit(X_train, newsgroups_train.target)\n",
    "    Y_predict = model_MNB.predict(X_test)\n",
    "    curr_accuracy = accuracy_score(newsgroups_test.target, Y_predict)\n",
    "    model_accuracy_vocabulary.append([curr_accuracy, vocabulary_size])\n",
    "      \n",
    "\n",
    "model_accuracy_vocabulary = np.array(model_accuracy_vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(model_accuracy_vocabulary[:,1], model_accuracy_vocabulary[:,0])\n",
    "plt.xlabel('Vocabulary Size')\n",
    "plt.ylabel('Model Accuracy')\n",
    "plt.title('Model accuracy vs Vocabulary Size')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best resoult should be a vocabulary size of 12.500\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
